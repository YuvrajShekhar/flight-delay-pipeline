# =============================================================
# Flight Delay Prediction Pipeline — Docker Compose
# IU International University of Applied Sciences
# Module: Data Engineering (DLMDSEDE02) | Task 1 | Phase 2
# Author: Yuvraj Shekhar
#
# Starts all 6 microservices on a private bridge network.
# Usage: docker-compose up -d
# =============================================================


# ── Shared private network ────────────────────────────────────
networks:
  pipeline-net:
    driver: bridge

# ── Named volumes for persistent data ────────────────────────
volumes:
  postgres_data:
  minio_data:
  zookeeper_data:
  kafka_data:

# ── Services ──────────────────────────────────────────────────
services:

  # ── 1. Zookeeper (Kafka dependency) ───────────────────────
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    restart: unless-stopped
    networks:
      - pipeline-net
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ── 2. Apache Kafka (Data Ingestion) ──────────────────────
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - pipeline-net
    volumes:
      - kafka_data:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_DEFAULT_REPLICATION_FACTOR: ${KAFKA_REPLICATION_FACTOR:-1}
      KAFKA_NUM_PARTITIONS: ${KAFKA_PARTITIONS:-3}
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168        # retain messages 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka:9092"]
      interval: 15s
      timeout: 10s
      retries: 10

  # ── 3. MinIO (Data Lake Storage) ──────────────────────────
  minio:
    image: minio/minio:latest
    container_name: minio
    restart: unless-stopped
    networks:
      - pipeline-net
    volumes:
      - minio_data:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"   # S3 API endpoint
      - "9001:9001"   # MinIO web console
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 15s
      timeout: 5s
      retries: 5

  # ── 4. Apache Spark (Batch Processing) ────────────────────
  spark-master:
    image: apache/spark:3.5.3
    container_name: spark-master
    restart: unless-stopped
    networks:
      - pipeline-net
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8081:8080"
      - "7077:7077"

  spark-worker:
    image: apache/spark:3.5.3
    container_name: spark-worker
    restart: unless-stopped
    depends_on:
      - spark-master
    networks:
      - pipeline-net
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  # ── 5. PostgreSQL (Feature Store) ─────────────────────────
  postgres:
    image: postgres:15
    container_name: postgres
    restart: unless-stopped
    networks:
      - pipeline-net
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d   # auto-runs SQL on first start
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ── 6. Apache Airflow (Orchestration) ─────────────────────
  # Uses a custom Dockerfile that extends apache/airflow:2.7.0
  # with pyspark and psycopg2-binary pre-installed.
  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - pipeline-net
    volumes:
      - ./airflow/dags:/opt/airflow/dags          # DAG files hot-reloaded
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data                  # dataset files
      - ./spark/jobs:/opt/airflow/spark/jobs      # PySpark scripts
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      # Pass pipeline credentials through to DAGs
      KAFKA_BROKER: ${KAFKA_BROKER}
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET: ${MINIO_BUCKET}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DATA_LOCAL_PATH: ${DATA_LOCAL_PATH}
      MINIO_RAW_PREFIX: ${MINIO_RAW_PREFIX}
    ports:
      - "8080:8080"   # Airflow web UI
    command: bash -c "airflow db init && airflow users create --username admin --password yuvi@123 --firstname Yuvraj --lastname Shekhar --role Admin --email admin@flightdelay.local && airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    restart: unless-stopped
    depends_on:
      airflow-webserver:
        condition: service_healthy
    networks:
      - pipeline-net
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./spark/jobs:/opt/airflow/spark/jobs
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      KAFKA_BROKER: ${KAFKA_BROKER}
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET: ${MINIO_BUCKET}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DATA_LOCAL_PATH: ${DATA_LOCAL_PATH}
      MINIO_RAW_PREFIX: ${MINIO_RAW_PREFIX}
    command: airflow scheduler
